{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b369cd4f",
   "metadata": {},
   "source": [
    "# Clustering con K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8102",
   "metadata": {},
   "source": [
    "Se tiene un conjunto de datos de tipo texto, los cuales se busca agrupar mediante kmenas.\n",
    "\n",
    "|quotes|\n",
    "|------|\n",
    "|Graphics designers are most creative people|\n",
    "|Artificial Intelligence or AI is the last invention - humans could ever make|\n",
    "|Snooker is a billiards sport for normally two players.|\n",
    "|Snooker is played on a large (12 feet by 6 feet) table that is covered with a smooth green material.|\n",
    "|FOREX is the stock market for trading currencies|\n",
    "|Software Engineering is hotter and hotter topic in Silicon Valley|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554f171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60727a23",
   "metadata": {},
   "source": [
    "Instalar nltk\n",
    "```\n",
    "conda install nltk\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e64098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections #For fetching dictionary of labels & clusters\n",
    "import nltk #Natural Language Toolkit\n",
    "from nltk import word_tokenize #Word tokenization is the process of splitting a large sample of text into words.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer #Normalizing Sentences\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0028ce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tuteggito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tuteggito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/tuteggito/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Descargar archivos adicionales para NLP\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e5dc8",
   "metadata": {},
   "source": [
    "Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f720a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('../datasets/quotes/quotes.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d10e16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graphics designers are most creative people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial Intelligence or AI is the last inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Snooker is a billiards sport for normally two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snooker is played on a large (12 feet by 6 fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOREX is the stock market for trading currencies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Software Engineering is hotter and hotter topi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Love is blind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Snooker is popular in the United Kingdom and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The flying or operating of aircraft is known a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AI is likely to be either the best or worst th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Quotes\n",
       "0        Graphics designers are most creative people\n",
       "1  Artificial Intelligence or AI is the last inve...\n",
       "2  Snooker is a billiards sport for normally two ...\n",
       "3  Snooker is played on a large (12 feet by 6 fee...\n",
       "4   FOREX is the stock market for trading currencies\n",
       "5  Software Engineering is hotter and hotter topi...\n",
       "6                                      Love is blind\n",
       "7  Snooker is popular in the United Kingdom and m...\n",
       "8  The flying or operating of aircraft is known a...\n",
       "9  AI is likely to be either the best or worst th..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head(10) #Verificar que el archivo se haya cargado correctamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1e349",
   "metadata": {},
   "source": [
    "Convertir el dataframe a lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2559257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = sentences[\"Quotes\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c345ce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Graphics designers are most creative people',\n",
       " 'Artificial Intelligence or AI is the last invention - humans could ever make',\n",
       " 'Snooker is a billiards sport for normally two players.',\n",
       " 'Snooker is played on a large (12 feet by 6 feet) table that is covered with a smooth green material.',\n",
       " 'FOREX is the stock market for trading currencies',\n",
       " 'Software Engineering is hotter and hotter topic in Silicon Valley',\n",
       " 'Love is blind',\n",
       " 'Snooker is popular in the United Kingdom and many other countries',\n",
       " 'The flying or operating of aircraft is known as aviation.',\n",
       " 'AI is likely to be either the best or worst thing happen to humanity',\n",
       " 'Design is Intelligence made visible.',\n",
       " 'Falling in love is like being on drugs.',\n",
       " 'There is only one happiness in Life to Love and to be loved.',\n",
       " \"Boeing 777 is considered world's largest economical plane in the world of Aviation.\",\n",
       " 'Warren Buffet is famous for making good investments.He knows stock markets',\n",
       " 'The biggest of the many uses of aviation are in air travel and military aircraft.',\n",
       " 'All giant majors in Silicon Valley is focusing AI for their business productivity',\n",
       " 'Investing in stocks and trading with them are not that easy',\n",
       " \"Google will fulfill its mission only when its search engine is AI - complete You guys know what that means? That's Artificial Intelligence.\",\n",
       " 'Being in love is the number one reason why people wed.',\n",
       " 'Aviation refers to flying using an aircraft like an aeroplane.',\n",
       " 'Auomation is the biggest blessing given by Artificial Inteligence.',\n",
       " 'Graphics Designing is high rated freelance subject',\n",
       " 'Loving from a long distance actually strengthens a relationship',\n",
       " 'Real love is able to awaken your soul.',\n",
       " 'Stuart Bingham is a champion of Snooker',\n",
       " 'Software Engineer has average salary of $170K at Google',\n",
       " 'AI would have a low error rate compared to humans if coded properly. ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e982e53",
   "metadata": {},
   "source": [
    "Crear una función de tokenización\n",
    "\n",
    "La tokenización es un paso fundamental en el Procesamiento del Lenguaje Natural (PLN), ya que transforma el texto no estructurado en un formato que las máquinas pueden comprender y manejar. En esencia, la tokenización divide el texto en unidades más pequeñas (palabras, subpalabras o caracteres) llamadas tokens ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9474f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "  tokens = word_tokenize(text) # Se va a dividir el texto en palabras\n",
    "  stemmer = PorterStemmer()\n",
    "  # Eliminación de ejes morfológicos\n",
    "  tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847ba6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00faa6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'run', 'store', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"I am running to the store.\")) #Verificar que la función de tokenización funcione correctamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0319e9",
   "metadata": {},
   "source": [
    "Definir la función para ejecutar el clústering de oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3d425",
   "metadata": {},
   "source": [
    "Entrenar un modelo de K-means\n",
    "Creación de la matriz vectorizadora tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dea3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sentences(sentences_list, k):\n",
    "\n",
    "  # Se crea la instancia tf-ifd quitando las stop words\n",
    "  # TfidfVectorizer se utiliza para convertir una colección de documentos sin procesar en una matriz de características TF-IDF.\n",
    "  tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer, stop_words=stopwords.words('english'),lowercase=True, token_pattern=None)\n",
    "\n",
    "  # Se crea la matriz vectorizadora para las oraciones\n",
    "  # Transforma el texto en vectores de características que pueden usarse como entrada para el estimador.\n",
    "  tfidf_matrix = tfidf_vectorizer.fit_transform(sentences_list)\n",
    "  print(tfidf_matrix) #Verificar que la matriz se haya creado correctamente\n",
    "\n",
    "  kmeans = KMeans(n_clusters=k)\n",
    "  kmeans.fit(tfidf_matrix)\n",
    "\n",
    "  clusters = collections.defaultdict(list)\n",
    "\n",
    "  for i, label in enumerate(kmeans.labels_):\n",
    "    clusters[label].append(i)\n",
    "\n",
    "  return dict(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a923432",
   "metadata": {},
   "source": [
    "Probar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff9f85dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 200 stored elements and shape (28, 138)>\n",
      "  Coords\tValues\n",
      "  (0, 62)\t0.49415409169888774\n",
      "  (0, 41)\t0.45066279882410365\n",
      "  (0, 39)\t0.5554516262200696\n",
      "  (0, 99)\t0.49415409169888774\n",
      "  (1, 17)\t0.2859764225487876\n",
      "  (1, 71)\t0.2859764225487876\n",
      "  (1, 14)\t0.24707893715800808\n",
      "  (1, 80)\t0.3524721130295035\n",
      "  (1, 72)\t0.3524721130295035\n",
      "  (1, 4)\t0.3135746276387239\n",
      "  (1, 69)\t0.2859764225487876\n",
      "  (1, 36)\t0.3524721130295035\n",
      "  (1, 49)\t0.3524721130295035\n",
      "  (1, 88)\t0.3135746276387239\n",
      "  (2, 115)\t0.31255512990364265\n",
      "  (2, 24)\t0.4164007148801905\n",
      "  (2, 118)\t0.4164007148801905\n",
      "  (2, 95)\t0.4164007148801905\n",
      "  (2, 128)\t0.4164007148801905\n",
      "  (2, 102)\t0.4164007148801905\n",
      "  (2, 5)\t0.18804657403751973\n",
      "  (3, 115)\t0.18903214915171598\n",
      "  (3, 5)\t0.11372984996883059\n",
      "  (3, 101)\t0.2518375624370003\n",
      "  (3, 78)\t0.2518375624370003\n",
      "  :\t:\n",
      "  (24, 107)\t0.4643250517595315\n",
      "  (24, 11)\t0.4643250517595315\n",
      "  (24, 21)\t0.4643250517595315\n",
      "  (24, 117)\t0.4643250517595315\n",
      "  (25, 115)\t0.39763244241181206\n",
      "  (25, 121)\t0.5297447312123129\n",
      "  (25, 25)\t0.5297447312123129\n",
      "  (25, 31)\t0.5297447312123129\n",
      "  (26, 116)\t0.35610794717827554\n",
      "  (26, 47)\t0.32476631652917454\n",
      "  (26, 61)\t0.35610794717827554\n",
      "  (26, 19)\t0.4002814945638322\n",
      "  (26, 111)\t0.4002814945638322\n",
      "  (26, 0)\t0.4002814945638322\n",
      "  (26, 7)\t0.4002814945638322\n",
      "  (27, 14)\t0.24561983896260717\n",
      "  (27, 69)\t0.28428761942025016\n",
      "  (27, 5)\t0.15823641645111672\n",
      "  (27, 106)\t0.31172284626644703\n",
      "  (27, 137)\t0.35039062672409005\n",
      "  (27, 85)\t0.35039062672409005\n",
      "  (27, 48)\t0.35039062672409005\n",
      "  (27, 33)\t0.35039062672409005\n",
      "  (27, 32)\t0.35039062672409005\n",
      "  (27, 105)\t0.35039062672409005\n",
      "\n",
      "CLUSTER  0 :\n",
      "\n",
      "\t 1 :  Graphics designers are most creative people\n",
      "\t 2 :  Software Engineering is hotter and hotter topic in Silicon Valley\n",
      "\t 3 :  Boeing 777 is considered world's largest economical plane in the world of Aviation.\n",
      "\t 4 :  Google will fulfill its mission only when its search engine is AI - complete You guys know what that means? That's Artificial Intelligence.\n",
      "\t 5 :  Graphics Designing is high rated freelance subject\n",
      "\t 6 :  Software Engineer has average salary of $170K at Google\n",
      "\n",
      "CLUSTER  1 :\n",
      "\n",
      "\t 1 :  Loving from a long distance actually strengthens a relationship\n",
      "\n",
      "CLUSTER  2 :\n",
      "\n",
      "\t 1 :  FOREX is the stock market for trading currencies\n",
      "\t 2 :  Warren Buffet is famous for making good investments.He knows stock markets\n",
      "\t 3 :  Investing in stocks and trading with them are not that easy\n",
      "\n",
      "CLUSTER  3 :\n",
      "\n",
      "\t 1 :  Love is blind\n",
      "\t 2 :  Design is Intelligence made visible.\n",
      "\t 3 :  Falling in love is like being on drugs.\n",
      "\t 4 :  There is only one happiness in Life to Love and to be loved.\n",
      "\t 5 :  Being in love is the number one reason why people wed.\n",
      "\t 6 :  Auomation is the biggest blessing given by Artificial Inteligence.\n",
      "\t 7 :  Real love is able to awaken your soul.\n",
      "\n",
      "CLUSTER  4 :\n",
      "\n",
      "\t 1 :  The flying or operating of aircraft is known as aviation.\n",
      "\t 2 :  The biggest of the many uses of aviation are in air travel and military aircraft.\n",
      "\t 3 :  Aviation refers to flying using an aircraft like an aeroplane.\n",
      "\n",
      "CLUSTER  5 :\n",
      "\n",
      "\t 1 :  Artificial Intelligence or AI is the last invention - humans could ever make\n",
      "\t 2 :  AI is likely to be either the best or worst thing happen to humanity\n",
      "\t 3 :  All giant majors in Silicon Valley is focusing AI for their business productivity\n",
      "\t 4 :  AI would have a low error rate compared to humans if coded properly. \n",
      "\n",
      "CLUSTER  6 :\n",
      "\n",
      "\t 1 :  Snooker is a billiards sport for normally two players.\n",
      "\t 2 :  Snooker is played on a large (12 feet by 6 feet) table that is covered with a smooth green material.\n",
      "\t 3 :  Snooker is popular in the United Kingdom and many other countries\n",
      "\t 4 :  Stuart Bingham is a champion of Snooker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuteggito/miniconda3/envs/machine-learning-examples/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k = 7\n",
    "clusters = cluster_sentences(sentences_list,k)\n",
    "for cluster in range (k):\n",
    "  print(\"\\nCLUSTER \",cluster,\":\\n\")\n",
    "  for i, sentence in enumerate(clusters[cluster]):\n",
    "    print(\"\\t\",(i+1),\": \",sentences_list[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ef4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
